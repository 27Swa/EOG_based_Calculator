{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1332ff2b-68c9-47c9-acab-ba8f88ae03ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pywt import wavedec\n",
    "import pywt\n",
    "from scipy.signal import butter, filtfilt\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import signal\n",
    "from scipy.signal import periodogram\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfafa981-a4ef-43c7-9d1a-686252d2ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: class-20250510T171956Z-1-001/class/Test\\Blink\\kirp17h.txt, Label: 4\n",
      "Saved: kirp17h.txt and kirp17v.txt — Label 4\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Blink\\kirp18h.txt, Label: 4\n",
      "Saved: kirp18h.txt and kirp18v.txt — Label 4\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Blink\\kirp19h.txt, Label: 4\n",
      "Saved: kirp19h.txt and kirp19v.txt — Label 4\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Blink\\kirp20h.txt, Label: 4\n",
      "Saved: kirp20h.txt and kirp20v.txt — Label 4\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Down\\asagi17h.txt, Label: 1\n",
      "Saved: asagi17h.txt and asagi17v.txt — Label 1\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Down\\asagi18h.txt, Label: 1\n",
      "Saved: asagi18h.txt and asagi18v.txt — Label 1\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Down\\asagi19h.txt, Label: 1\n",
      "Saved: asagi19h.txt and asagi19v.txt — Label 1\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Down\\asagi20h.txt, Label: 1\n",
      "Saved: asagi20h.txt and asagi20v.txt — Label 1\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Left\\sol17h.txt, Label: 3\n",
      "Saved: sol17h.txt and sol17v.txt — Label 3\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Left\\sol18h.txt, Label: 3\n",
      "Saved: sol18h.txt and sol18v.txt — Label 3\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Left\\sol19h.txt, Label: 3\n",
      "Saved: sol19h.txt and sol19v.txt — Label 3\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Left\\sol20h.txt, Label: 3\n",
      "Saved: sol20h.txt and sol20v.txt — Label 3\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Right\\sag17h.txt, Label: 2\n",
      "Saved: sag17h.txt and sag17v.txt — Label 2\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Right\\sag18h.txt, Label: 2\n",
      "Saved: sag18h.txt and sag18v.txt — Label 2\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Right\\sag19h.txt, Label: 2\n",
      "Saved: sag19h.txt and sag19v.txt — Label 2\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Right\\sag20h.txt, Label: 2\n",
      "Saved: sag20h.txt and sag20v.txt — Label 2\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Up\\yukari17h.txt, Label: 0\n",
      "Saved: yukari17h.txt and yukari17v.txt — Label 0\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Up\\yukari18h.txt, Label: 0\n",
      "Saved: yukari18h.txt and yukari18v.txt — Label 0\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Up\\yukari19h.txt, Label: 0\n",
      "Saved: yukari19h.txt and yukari19v.txt — Label 0\n",
      "Processing: class-20250510T171956Z-1-001/class/Test\\Up\\yukari20h.txt, Label: 0\n",
      "Saved: yukari20h.txt and yukari20v.txt — Label 0\n",
      "\n",
      "=== Summary ===\n",
      "up (0): 4 samples\n",
      "down (1): 4 samples\n",
      "right (2): 4 samples\n",
      "left (3): 4 samples\n",
      "blink (4): 4 samples\n",
      "\n",
      "Unique labels in Y_h: [4 1 3 2 0]\n",
      "Unique labels in Y_v: [4 1 3 2 0]\n",
      "\n",
      "Sample count per label (horizontal):\n",
      "251\n",
      "4    4\n",
      "1    4\n",
      "3    4\n",
      "2    4\n",
      "0    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample count per label (vertical):\n",
      "251\n",
      "4    4\n",
      "1    4\n",
      "3    4\n",
      "2    4\n",
      "0    4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train_data=\"C:/Users/LENOVO/Downloads/class-20250510T171956Z-1-001/class/Train\"\n",
    "test_data='class-20250510T171956Z-1-001/class/Test'\n",
    "# horizontal_file_train=\"data_train_h.csv\"\n",
    "# vertical_file_train=\"data_train_v.csv\"\n",
    "horizontal_file_test=\"data_test_h.csv\"\n",
    "vertical_file_test=\"data_test_v.csv\"\n",
    "def read_and_process_signal_files(root_dir, horizontal_file, vertical_file):\n",
    "    # Map folder names to labels\n",
    "    class_labels = {\n",
    "        'up': 0,\n",
    "        'down': 1,\n",
    "        'right': 2,\n",
    "        'left': 3,\n",
    "        'blink': 4\n",
    "    }\n",
    "\n",
    "    # Counters\n",
    "    sample_counts = {label: 0 for label in class_labels.values()}\n",
    "    skipped_files = []\n",
    "\n",
    "    with open(horizontal_file, 'w') as csv_h, open(vertical_file, 'w') as csv_v:\n",
    "        # Loop over each subfolder\n",
    "        for subdir_name in os.listdir(root_dir):\n",
    "            subdir_path = os.path.join(root_dir, subdir_name)\n",
    "            if not os.path.isdir(subdir_path):\n",
    "                continue\n",
    "\n",
    "            class_key = subdir_name.lower()\n",
    "            if class_key not in class_labels:\n",
    "                print(f\"Skipping unknown class folder: {subdir_name}\")\n",
    "                continue\n",
    "\n",
    "            label = class_labels[class_key]\n",
    "\n",
    "            # Search for horizontal signal files\n",
    "            h_files = glob.glob(os.path.join(subdir_path, \"*h.txt\"))\n",
    "\n",
    "            for h_file in h_files:\n",
    "                v_file = h_file.replace('h.txt', 'v.txt')\n",
    "                if not os.path.exists(v_file):\n",
    "                    print(f\"Missing v.txt for: {h_file}\")\n",
    "                    skipped_files.append((h_file, \"missing v.txt\"))\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(h_file, 'r') as f_h, open(v_file, 'r') as f_v:\n",
    "                        h_raw = f_h.read().strip()\n",
    "                        v_raw = f_v.read().strip()\n",
    "\n",
    "                        if not h_raw or not v_raw:\n",
    "                            print(f\"Skipping empty file(s): {h_file}, {v_file}\")\n",
    "                            skipped_files.append((h_file, \"empty file\"))\n",
    "                            continue\n",
    "\n",
    "                        h_lines = h_raw.replace('\\n', ',').strip(',')\n",
    "                        v_lines = v_raw.replace('\\n', ',').strip(',')\n",
    "\n",
    "                        # Debug print\n",
    "                        print(f\"Processing: {h_file}, Label: {label}\")\n",
    "\n",
    "                        # Write to CSVs\n",
    "                        csv_h.write(h_lines + ',' + str(label) + '\\n')\n",
    "                        csv_v.write(v_lines + ',' + str(label) + '\\n')\n",
    "\n",
    "                        sample_counts[label] += 1\n",
    "                        print(f\"Saved: {os.path.basename(h_file)} and {os.path.basename(v_file)} — Label {label}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading files: {h_file}, {v_file}\\n{e}\")\n",
    "                    skipped_files.append((h_file, str(e)))\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    for label, count in sample_counts.items():\n",
    "        class_name = [k for k, v in class_labels.items() if v == label][0]\n",
    "        print(f\"{class_name} ({label}): {count} samples\")\n",
    "\n",
    "    if skipped_files:\n",
    "        print(\"\\nSkipped file details:\")\n",
    "        for file, reason in skipped_files:\n",
    "            print(f\"{file} — {reason}\")\n",
    "\n",
    "    # Load CSVs into DataFrames\n",
    "    df_h = pd.read_csv(horizontal_file, header=None)\n",
    "    df_v = pd.read_csv(vertical_file, header=None)\n",
    "\n",
    "    # Display label distribution\n",
    "    Y_h = df_h.iloc[:, -1]\n",
    "    Y_v = df_v.iloc[:, -1]\n",
    "\n",
    "    print(f\"\\nUnique labels in Y_h: {Y_h.unique()}\")\n",
    "    print(f\"Unique labels in Y_v: {Y_v.unique()}\")\n",
    "\n",
    "    print(\"\\nSample count per label (horizontal):\")\n",
    "    print(Y_h.value_counts())\n",
    "\n",
    "    print(\"\\nSample count per label (vertical):\")\n",
    "    print(Y_v.value_counts())\n",
    "\n",
    "    return df_h, df_v\n",
    "# dfTrain_h, dfTrain_v = read_and_process_signal_files(train_data,horizontal_file_train,vertical_file_train)\n",
    "dfTest_h, dfTest_v=read_and_process_signal_files(test_data,horizontal_file_test,vertical_file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a0edee-8a51-48d5-a289-c9249f36ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_h shape: (20, 251)\n",
      "X_v shape: (20, 251)\n",
      "\n",
      "Unique labels in Y_h: [4 1 3 2 0]\n",
      "Unique labels in Y_v: [4 1 3 2 0]\n",
      "\n",
      "Sample count per label (horizontal):\n",
      "251\n",
      "0    4\n",
      "1    4\n",
      "2    4\n",
      "3    4\n",
      "4    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample count per label (vertical):\n",
      "251\n",
      "0    4\n",
      "1    4\n",
      "2    4\n",
      "3    4\n",
      "4    4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split into features and labels\n",
    "X_h_test = dfTest_h.iloc[:, :-1]  # All columns except the last\n",
    "X_v_test = dfTest_v.iloc[:, :-1] \n",
    "Y_h_test = dfTest_h.iloc[:, -1].astype(int)  # Last column as integer labels\n",
    "Y_v_test = dfTest_v.iloc[:, -1].astype(int)\n",
    "\n",
    "# Show shapes\n",
    "print(\"\\nX_h shape:\", X_h_test.shape)\n",
    "print(\"X_v shape:\", X_v_test.shape)\n",
    "\n",
    "# Show unique labels\n",
    "print(\"\\nUnique labels in Y_h:\", Y_h_test.unique())\n",
    "print(\"Unique labels in Y_v:\", Y_v_test.unique())\n",
    "\n",
    "# Count samples per label\n",
    "print(\"\\nSample count per label (horizontal):\")\n",
    "print(Y_h_test.value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSample count per label (vertical):\")\n",
    "print(Y_v_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5027b93f-2638-4d02-b868-49e39d46562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(Input_Signal,LOW_Cutoff,High_cuttOff,Sampling_Rate,order):\n",
    "    nyq = 0.5 *Sampling_Rate\n",
    "    low = LOW_Cutoff/ nyq\n",
    "    high = High_cuttOff / nyq\n",
    "    Numerator,denominator = butter(order,[low,high],btype = \"band\",output = \"ba\",analog = False,fs = None)\n",
    "    filtered = filtfilt(Numerator,denominator,Input_Signal)\n",
    "    return filtered\n",
    "\n",
    "# def resample_filter(Filtered_Data):\n",
    "#     resampled_Signal = []\n",
    "#     for i in Filtered_Data:\n",
    "#         re_Sgnl = signal.resample(i,50)\n",
    "#         resampled_Signal.append(re_Sgnl)\n",
    "#     return resampled_Signal\n",
    "\n",
    "# def remove_dc(signal_array):\n",
    "#     return signal_array - np.mean(signal_array, axis=1, keepdims=True)\n",
    "\n",
    "# # 4. Normalization (Z-score)\n",
    "# def normalize(signal_array):\n",
    "#     mean = np.mean(signal_array, axis=1, keepdims=True)\n",
    "#     std = np.std(signal_array, axis=1, keepdims=True)\n",
    "#     return (signal_array - mean) / std\n",
    "\n",
    "# cutoff = 10  # Choose your cutoff frequency\n",
    "filtered_Signal_h_test = butter_bandpass_filter(X_h_test,LOW_Cutoff=.5,High_cuttOff=20,Sampling_Rate=176,order=2)\n",
    "filtered_Signal_v_test = butter_bandpass_filter(X_v_test,LOW_Cutoff=0.5,High_cuttOff=20,Sampling_Rate=176,order=2)\n",
    "# res_rsample_h=resample_filter(filtered_Signal_h)\n",
    "\n",
    "# normalized_h = normalize(zero_mean_h)\n",
    "\n",
    "\n",
    "# filtered_Signal_v = butter_bandpass_filter(X_v,LOW_Cutoff=0.5,High_cuttOff=20,Sampling_Rate=176,order=4)\n",
    "\n",
    "# res_rsample_v =resample_filter(filtered_Signal_v)\n",
    "\n",
    "# zero_mean_v = remove_dc(res_rsample_v)\n",
    "\n",
    "\n",
    "# normalized_v = normalize(zero_mean_v)\n",
    "\n",
    "\n",
    "X_test_combined = np.concatenate((filtered_Signal_h_test, filtered_Signal_v_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26a0772-6271-4c26-80a7-fe6452ce0fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------- Morphological Feature Extraction ----------\n",
    "def extract_morphological_features(signal_data):\n",
    "    features = []\n",
    "    for signal in signal_data:\n",
    "        # Wavelength\n",
    "        wavelength = np.sum(np.abs(np.diff(signal)))\n",
    "\n",
    "        # Peaks and valleys\n",
    "        peaks, _ = find_peaks(signal)\n",
    "        valleys, _ = find_peaks(-signal)\n",
    "\n",
    "        # Peak amplitude and position\n",
    "        if peaks.size > 0:\n",
    "            peak_idx = np.argmax(signal[peaks])\n",
    "            peak_amp = signal[peaks[peak_idx]]\n",
    "            peak_pos = peaks[peak_idx]\n",
    "        else:\n",
    "            peak_amp = 0\n",
    "            peak_pos = 0\n",
    "\n",
    "        # Valley amplitude and position\n",
    "        if valleys.size > 0:\n",
    "            valley_idx = np.argmin(signal[valleys])\n",
    "            valley_amp = signal[valleys[valley_idx]]\n",
    "            valley_pos = valleys[valley_idx]\n",
    "        else:\n",
    "            valley_amp = 0\n",
    "            valley_pos = 0\n",
    "\n",
    "        # Area under curve (absolute)\n",
    "        area = np.trapz(np.abs(signal))\n",
    "\n",
    "        # Append all features\n",
    "        features.append([\n",
    "            wavelength, peak_amp, valley_amp, area,\n",
    "            peak_pos, valley_pos\n",
    "        ])\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d56bb8a-921c-43b6-9e16-aede4708c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h_features_test = extract_morphological_features(filtered_Signal_h_test)\n",
    "v_features_test = extract_morphological_features(filtered_Signal_v_test)\n",
    "morph_features_test = np.concatenate([h_features_test, v_features_test], axis=1)\n",
    "\n",
    "# Feature labels\n",
    "columns = [\n",
    "    'Wavelength (H)', 'Peak Amplitude (H)', 'Valley Amplitude (H)', 'Area Under Curve (H)','Peak Position (H)','Valley Position (H)',\n",
    "    'Wavelength (V)', 'Peak Amplitude (V)', 'Valley Amplitude (V)', 'Area Under Curve (V)','Peak Position (V)','Valley Position (V)'\n",
    "]\n",
    "\n",
    "morph_df_test = pd.DataFrame(morph_features_test, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f091bbf7-66e2-4203-9d72-d61878589c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Peak Amplitude (H)', 'Peak Position (H)','Valley Position (H)', 'Peak Amplitude (V)','Peak Position (V)','Valley Position (V)']\n",
    "selected_features_df_test = morph_df_test[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518d1232-ecc2-4b5c-a093-63f5efeaaee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_loaded = joblib.load(\"Morphological Feature model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "501a7c5c-6e16-4775-b629-5cdaf921085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Model Test Accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get predictions\n",
    "predictions = svm_model_loaded.predict(selected_features_df_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(Y_h_test, predictions)\n",
    "print(f\"✅ Loaded Model Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777a5185-6055-4261-92c6-96107892c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the label mapping\n",
    "label_map = {\n",
    "    0: 'up',\n",
    "    1: 'down',\n",
    "    2: 'right',\n",
    "    3: 'left',\n",
    "    4: 'blink'\n",
    "}\n",
    "\n",
    "# Step 2: Map predicted and true labels\n",
    "mapped_true_labels = [label_map[label] for label in Y_h_test]\n",
    "mapped_predicted_labels = [label_map[label] for label in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6771cf2a-42d5-4825-a090-01565f8b257c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped True Labels: ['blink', 'blink', 'blink', 'blink', 'down', 'down', 'down', 'down', 'left', 'left', 'left', 'left', 'right', 'right', 'right']\n",
      "Mapped Predicted Labels: ['blink', 'up', 'blink', 'blink', 'down', 'down', 'down', 'down', 'left', 'left', 'left', 'left', 'right', 'right', 'right']\n"
     ]
    }
   ],
   "source": [
    "print(\"Mapped True Labels:\", mapped_true_labels[0:15])\n",
    "print(\"Mapped Predicted Labels:\", mapped_predicted_labels[0:15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
